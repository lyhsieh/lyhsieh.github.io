<!DOCTYPE html>
<html>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sparse and Privacy-enhanced Representation for Human Pose Estimation">
  <meta name="keywords" content="Human pose estimation, motion vector, privacy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sparse and Privacy-enhanced Representation for Human Pose Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://lyhsieh.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Sparse and Privacy-enhanced Representation for Human Pose Estimation</h1>
          <h2 class="title is-size-3 publication-title">BMVC 2023</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Ting-Ying Lin*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://lyhsieh.github.io">Lin-Yung Hsieh*</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fuenwang.phd/">Fu-En Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Wen-Shen Wuen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://aliensunmin.github.io">Min Sun</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Tsing Hua University</span>
            <span class="author-block"><sup>2</sup>Novatek Microelectronics Corp.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2309.09515"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lyhsieh/SPHP"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- DATASET Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-light fa-folder-open"></i>
                  </span>
                  <span>Dataset (coming soon)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <center><img src="static/images/sphp.gif" width="70%"></center>
        <div class="content has-text-justified">
          <br>
          <p>
            SPHP dataset. Our motion vector sensor (MVS) extracts edge images and two-directional motion vector images at each time frame. We also provide annotated body joints for human poses and corresponding grayscale images.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>

  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose a sparse and privacy-enhanced representation for Human 
            Pose Estimation (HPE). Given a perspective camera, we use a 
            proprietary motion vector sensor(MVS) to extract an edge image and 
            a two-directional motion vector image at each time frame. Both edge 
            and motion vector images are sparse and contain much less information 
            (i.e., enhancing human privacy). We advocate that edge information 
            is essential for HPE, and motion vectors complement edge information 
            during fast movements. 
            <br>
            <br>
            We propose a fusion network leveraging recent 
            advances in sparse convolution used typically for 3D voxels to 
            efficiently process our proposed sparse representation, which achieves 
            about 13x speed-up and 96% reduction in FLOPs. We collect an in-house 
            edge and motion vector dataset with 16 types of actions by 40 users 
            using the proprietary MVS. Our method outperforms individual modalities 
            using only edge or motion vector images. Finally, we validate the 
            privacy-enhanced quality of our sparse representation through face 
            recognition on CelebA (a large face dataset) and a user study on our 
            in-house dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Overview. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ImGeoNet</h2>
        <center><img src="./static/images/overview.png"></center>
        <div class="content has-text-justified">
          <br>
          <p>
            Given an arbitrary number of images, a 2D convolution backbone (Conv2D) is applied to extract visual features from each image, and then a 3D voxel feature volume is constructed by back-projecting and accumulating 2D features to the volume.
            This feature volume is not ideal since the underlying geometry of the scene is not considered.
            Hence, the proposed geometry shaping is applied to weight the original feature volume by the predicted surface probabilities, which preserves the geometric structure and removes voxels of free space.
            Finally, the geometry-aware volume is passed to the multiscale 3D convolutional layers (M3DConv) and the detection head.
          </p>
        </div>
      </div>
    </div> -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">SPHP Dataset</h2>
        <div class="table_wrapper">
          <style type="text/css">
            .tg  {border-collapse:collapse;border-color:#bbb;border-spacing:0;}
            .tg td{background-color:#E0FFEB;border-color:#bbb;border-style:solid;border-width:1px;color:#594F4F;
              font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg th{background-color:#9DE0AD;border-color:#bbb;border-style:solid;border-width:1px;color:#493F3F;
              font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg .tg-baqh{text-align:center;vertical-align:top}
            .tg .tg-8d8j{text-align:center;vertical-align:bottom}
            </style>
            <center>
            <table class="tg">
            <thead>
              <tr>
                <th class="tg-baqh"><b>ID#</b></th>
                <th class="tg-baqh"><b>Class#</b></th>
                <th class="tg-baqh"><b>Movement</b></th>
                <th class="tg-baqh"><b>ID#</b></th>
                <th class="tg-baqh"><b>Class#</b></th>
                <th class="tg-baqh"><b>Movement</b></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="tg-baqh">1</td>
                <td class="tg-8d8j">1</td>
                <td class="tg-baqh">Arm abduction</td>
                <td class="tg-baqh">9</td>
                <td class="tg-baqh">3</td>
                <td class="tg-baqh">Walk in place</td>
              </tr>
              <tr>
                <td class="tg-baqh">2</td>
                <td class="tg-8d8j">1</td>
                <td class="tg-baqh">Arm bicep curl</td>
                <td class="tg-baqh">10</td>
                <td class="tg-baqh">3</td>
                <td class="tg-baqh">Squat</td>
              </tr>
              <tr>
                <td class="tg-baqh">3</td>
                <td class="tg-8d8j">1</td>
                <td class="tg-baqh">Wave hello</td>
                <td class="tg-baqh">11</td>
                <td class="tg-baqh">4</td>
                <td class="tg-baqh">Jump in place</td>
              </tr>
              <tr>
                <td class="tg-baqh">4</td>
                <td class="tg-8d8j">1</td>
                <td class="tg-baqh">Punch up forward</td>
                <td class="tg-baqh">12</td>
                <td class="tg-baqh">4</td>
                <td class="tg-baqh">Jog in place</td>
              </tr>
              <tr>
                <td class="tg-baqh">5</td>
                <td class="tg-8d8j">2</td>
                <td class="tg-baqh">Leg knee lift</td>
                <td class="tg-baqh">13</td>
                <td class="tg-baqh">4</td>
                <td class="tg-baqh">Jumping jack</td>
              </tr>
              <tr>
                <td class="tg-baqh">6</td>
                <td class="tg-8d8j">2</td>
                <td class="tg-baqh">Leg abduction</td>
                <td class="tg-baqh">14</td>
                <td class="tg-baqh">3</td>
                <td class="tg-baqh">Standing side bend</td>
              </tr>
              <tr>
                <td class="tg-baqh">7</td>
                <td class="tg-8d8j">2</td>
                <td class="tg-baqh">Leg pulling</td>
                <td class="tg-baqh">15</td>
                <td class="tg-baqh">4</td>
                <td class="tg-baqh">Hop on one foot</td>
              </tr>
              <tr>
                <td class="tg-baqh">8</td>
                <td class="tg-baqh">4</td>
                <td class="tg-baqh">Elbow-to-knee</td>
                <td class="tg-baqh">16</td>
                <td class="tg-baqh">3</td>
                <td class="tg-baqh">Roll wrists &amp; ankles</td>
              </tr>
            </tbody>
            </table>
            </center>
        </div>
        <div class="content has-text-justified">
          <p>
            <br>
            We collected data from 40 subjects, having a balanced distribution 
            of 20 male and 20 female participants. Participants performed 16 
            fitness-related actions, which are categorized into four classes 
            based on the type of movement: 
            <ul>
              <li>C1: upper-body movements</li>
              <li>C2: lower-body movements</li>
              <li>C3: slow whole-body movements</li>
              <li>C4: fast whole-body movements</li>
            </ul>
            To diversify the viewing angles of our dataset, we apply a novel 
            strategy to capture each action from multiple perspectives. 
            Firstly, we place two cameras within an interval of 45 degrees. 
            Then, we instruct the participants to face various directions 
            (i.e., 0, 15, 30, and 45 degrees, respectively) while capturing 
            their actions. In each direction, every participant will perform 
            four actions, totaling 16 actions, as listed in the table above.
          </p>
        </div>
      </div>
    </div>
    <!-- Result. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <center><img src="./static/images/visualization.jpg" width="100%"></center>
      </div>
    </div> -->

    <!-- Fusion Model -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Fusion Model</h2>
        <center><img src="static/images/early_fusion3d.png" width="70%"></center>
          <div class="content has-text-justified">
            <p>
              Edge and motion vector information complement each other. While edge 
              is sufficient for detecting clear and non-blurred body joints, 
              incorporating motion vectors into our model can effectively address 
              the challenges posed by fast movements and overlapping boundaries, 
              which may confuse edge-based HPE models. 
              <br>
              <br>
              Hence, we aim to combine the complementary information of edge and 
              motion vectors while keeping our model compact and efficient. We 
              directly concatenate an edge image and a two-directional motion 
              vector image, proposing the early fusion model (referred to as FS) 
              as illustrated in the figure below. Our FS model can leverage 
              various single-branch network architectures designed for compactness 
              and efficiency.
            </p>
          </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{lin2023sparse,
  title     = {Sparse and Privacy-enhanced Representation for Human Pose Estimation},
  author    = {Lin, Ting-Ying and Hsieh, Lin-Yung and Wang, Fu-En and Wuen, Wen-Shen and Sun, Min},
  booktitle = {British Machine Vision Conference (BMVC)},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>